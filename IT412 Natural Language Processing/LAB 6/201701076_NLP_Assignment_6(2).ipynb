{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KprC_Ej5Kkj7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import nltk\n",
    "import string\n",
    "import math\n",
    "from pycontractions import Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k8LIi6WrLf0v"
   },
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Vedant/Desktop/NLP LAB/N-gram dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = 'C:/Users/Vedant/Desktop/NLP LAB/N-gram dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9mAdquULdWb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for y in range(4,7):\n",
    "    path = os.path.join(pat,'200'+ str(y))\n",
    "    os.chdir(path)\n",
    "    for filename in os.listdir():\n",
    "        f=0\n",
    "        with io.open(filename,'r') as f:\n",
    "            file = f.read()\n",
    "        if 'index' not in filename:\n",
    "            for st in re.split('\\n',file):\n",
    "                if '200'+str(y) in st:\n",
    "                    f=1\n",
    "                    continue\n",
    "                if not (st == '' or st[0] == '<'):\n",
    "                    if f == 1:\n",
    "                        Data.append(st)\n",
    "        else:\n",
    "            for st in re.split('\\n',file):\n",
    "                if '1999' in st:\n",
    "                    f=1\n",
    "                    continue\n",
    "                if not (st == '' or st[0] == '<'):\n",
    "                    if f == 1:\n",
    "                        Data.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 7\n",
    "path = os.path.join(pat,'200'+ str(y))\n",
    "os.chdir(path)\n",
    "for filename in os.listdir():\n",
    "    f=0\n",
    "    with io.open(filename,'r') as f:\n",
    "        file = f.read()\n",
    "    if 'index' not in filename:\n",
    "        for st in re.split('\\n',file):\n",
    "            if '200'+str(y) in st:\n",
    "                f=1\n",
    "                continue\n",
    "            if not (st == '' or st[0] == '<'):\n",
    "                if f == 1:\n",
    "                    test.append(st)\n",
    "    else:\n",
    "        for st in re.split('\\n',file):\n",
    "            if '1999' in st:\n",
    "                f=1\n",
    "                continue\n",
    "            if not (st == '' or st[0] == '<'):\n",
    "                if f == 1:\n",
    "                    test.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramLanguageModel:\n",
    "    def __init__(self, sentences, smoothing=False):\n",
    "        self.unigram_frequencies = dict()\n",
    "        self.corpus_length = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                self.unigram_frequencies[word] = self.unigram_frequencies.get(word, 0) + 1\n",
    "                self.corpus_length += 1\n",
    "        self.unique_words = len(self.unigram_frequencies)\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def calculate_unigram_probability(self, word):\n",
    "            word_probability_numerator = self.unigram_frequencies.get(word, 0)\n",
    "            word_probability_denominator = self.corpus_length\n",
    "            if self.smoothing:\n",
    "                word_probability_numerator += 1\n",
    "                word_probability_denominator += self.unique_words + 1\n",
    "            return float(word_probability_numerator) / float(word_probability_denominator)\n",
    "\n",
    "    def calculate_sentence_probability(self, sentence, normalize_probability=True):\n",
    "        sentence_probability_log_sum = 0\n",
    "        for word in sentence:\n",
    "            word_probability = self.calculate_unigram_probability(word)\n",
    "            sentence_probability_log_sum += math.log(word_probability, 2)\n",
    "        return math.pow(2, sentence_probability_log_sum) if normalize_probability else sentence_probability_log_sum                \n",
    "\n",
    "    def sorted_vocabulary(self):\n",
    "        full_vocab = list(self.unigram_frequencies.keys())\n",
    "        full_vocab.sort()\n",
    "        full_vocab.append(UNK)\n",
    "        return full_vocab\n",
    "    \n",
    "class BigramLanguageModel(UnigramLanguageModel):\n",
    "    def __init__(self, sentences, smoothing=False):\n",
    "        UnigramLanguageModel.__init__(self, sentences, smoothing)\n",
    "        self.bigram_frequencies = dict()\n",
    "        self.unique_bigrams = set()\n",
    "        for sentence in sentences:\n",
    "            previous_word = None\n",
    "            for word in sentence:\n",
    "                if previous_word != None:\n",
    "                    self.bigram_frequencies[(previous_word, word)] = self.bigram_frequencies.get((previous_word, word),0) + 1\n",
    "                    self.unique_bigrams.add((previous_word, word))\n",
    "                previous_word = word\n",
    "        self.unique__bigram_words = len(self.unigram_frequencies)\n",
    "\n",
    "    def calculate_bigram_probabilty(self, previous_word, word):\n",
    "        bigram_word_probability_numerator = self.bigram_frequencies.get((previous_word, word), 0)\n",
    "        bigram_word_probability_denominator = self.unigram_frequencies.get(previous_word, 0)\n",
    "        if self.smoothing:\n",
    "            bigram_word_probability_numerator += 1\n",
    "            bigram_word_probability_denominator += self.unique__bigram_words\n",
    "        if bigram_word_probability_numerator == 0 or bigram_word_probability_denominator == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(bigram_word_probability_numerator) / float(bigram_word_probability_denominator)\n",
    "\n",
    "    def calculate_bigram_sentence_probability(self, sentence, normalize_probability=True):\n",
    "        bigram_sentence_probability_log_sum = 0\n",
    "        previous_word = None\n",
    "        for word in sentence:\n",
    "            if previous_word != None:\n",
    "                bigram_word_probability = self.calculate_bigram_probabilty(previous_word, word)\n",
    "                bigram_sentence_probability_log_sum += math.log(bigram_word_probability, 2)\n",
    "            previous_word = word\n",
    "        return math.pow(2,bigram_sentence_probability_log_sum) if normalize_probability else bigram_sentence_probability_log_sum\n",
    "\n",
    "class TrigramLanguageModel(BigramLanguageModel):\n",
    "    def __init__(self,sentences,smoothing = False):\n",
    "        BigramLanguageModel.__init__(self, sentences, smoothing)\n",
    "        self.trigram_frequencies = dict()\n",
    "        self.unique_trigrams = set()\n",
    "        for sentence in sentences:\n",
    "            first_word = None\n",
    "            second_word = None\n",
    "            for word in sentence:\n",
    "                if second_word != None and first_word != None:\n",
    "                    self.trigram_frequencies[(second_word,first_word, word)] = self.trigram_frequencies.get((second_word,first_word, word),0) + 1\n",
    "                    self.unique_trigrams.add((second_word,first_word, word))\n",
    "                second_word = first_word\n",
    "                first_word = word\n",
    "        self.unique__trigram_words = len(self.unigram_frequencies)\n",
    "    def calculate_trigram_probabilty(self,second_word,first_word,word):\n",
    "        trigram_word_probability_numerator = self.trigram_frequencies.get((second_word,first_word, word), 0)\n",
    "        trigram_word_probability_denominator = self.bigram_frequencies.get((second_word,first_word), 0)\n",
    "        if self.smoothing:\n",
    "            trigram_word_probability_numerator += 1\n",
    "            trigram_word_probability_denominator += self.unique__trigram_words\n",
    "        return 0.0 if trigram_word_probability_numerator == 0 or trigram_word_probability_denominator == 0 else float(\n",
    "            trigram_word_probability_numerator) / float(trigram_word_probability_denominator)\n",
    "\n",
    "    def calculate_trigram_sentence_probability(self, sentence, normalize_probability=True):\n",
    "        trigram_sentence_probability_log_sum = 0\n",
    "        second_word = None\n",
    "        first_word = None\n",
    "        \n",
    "        for word in sentence:\n",
    "            if second_word != None and first_word != None:\n",
    "                trigram_word_probability = self.calculate_trigram_probabilty(second_word,first_word, word)\n",
    "                trigram_sentence_probability_log_sum += math.log(trigram_word_probability, 2)\n",
    "            first_word = word\n",
    "            second_word = first_word\n",
    "        return math.pow(2,\n",
    "                        trigram_sentence_probability_log_sum) if normalize_probability else trigram_sentence_probability_log_sum\n",
    "\n",
    "    \n",
    "def calculate_number_of_unigrams(sentences):\n",
    "    unigram_count = 0\n",
    "    for sentence in sentences:\n",
    "        unigram_count += len(sentence)\n",
    "    return unigram_count\n",
    "def calculate_number_of_bigrams(sentences):\n",
    "        bigram_count = 0\n",
    "        for sentence in sentences:\n",
    "            bigram_count += len(sentence) - 1\n",
    "        return bigram_count\n",
    "def calculate_number_of_trigrams(sentences):\n",
    "        trigram_count = 0\n",
    "        for sentence in sentences:\n",
    "            trigram_count += len(sentence) - 2\n",
    "        return trigram_count\n",
    "def calculate_unigram_perplexity(model, sentences):\n",
    "    unigram_count = calculate_number_of_unigrams(sentences)\n",
    "    sentence_probability_log_sum = 0\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            sentence_probability_log_sum -= math.log(model.calculate_sentence_probability(sentence), 2)\n",
    "        except:\n",
    "            pass\n",
    "    return math.pow(2, sentence_probability_log_sum / unigram_count)\n",
    "\n",
    "def calculate_bigram_perplexity(model, sentences):\n",
    "    number_of_bigrams = calculate_number_of_bigrams(sentences)\n",
    "    bigram_sentence_probability_log_sum = 0\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            bigram_sentence_probability_log_sum -= math.log(model.calculate_bigram_sentence_probability(sentence), 2)\n",
    "        except:\n",
    "            pass\n",
    "    return math.pow(2, bigram_sentence_probability_log_sum / number_of_bigrams)\n",
    "\n",
    "def calculate_trigram_perplexity(model, sentences):\n",
    "    number_of_trigrams = calculate_number_of_trigrams(sentences)\n",
    "    trigram_sentence_probability_log_sum = 0\n",
    "    for sentence in sentences:\n",
    "        try:\n",
    "            trigram_sentence_probability_log_sum -= math.log(model.calculate_trigram_sentence_probability(sentence), 2)\n",
    "        except:\n",
    "            pass\n",
    "    return math.pow(2, trigram_sentence_probability_log_sum / number_of_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import digits\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "from string import punctuation\n",
    "table = str.maketrans('','',punctuation)\n",
    "cont = Contractions(api_key=\"glove-twitter-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(len(Data)):\n",
    "    s = Data[d]\n",
    "    s = s.lower()\n",
    "    s = s.translate(table)\n",
    "    s = s.translate(remove_digits)\n",
    "    s = list(cont.expand_texts([s]))[0]\n",
    "    s = nltk.word_tokenize(s)\n",
    "    Data[d] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_model = TrigramLanguageModel(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_model_smoothed = TrigramLanguageModel(Data,smoothing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocab_keys = dataset_model.sorted_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== TRAIN PERPLEXITY == \n",
      "unigram:  1535.4401039431868\n",
      "bigram:  82.7156875550523\n",
      "trigram:  1.0000052122827534\n"
     ]
    }
   ],
   "source": [
    "print(\"== TRAIN PERPLEXITY == \")\n",
    "print(\"unigram: \", calculate_unigram_perplexity(dataset_model, Data))\n",
    "print(\"bigram: \", calculate_bigram_perplexity(dataset_model, Data))\n",
    "print(\"trigram: \", calculate_trigram_perplexity(dataset_model, Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== TRAIN PERPLEXITY == \n",
      "unigram:  1545.508219559923\n",
      "bigram:  3416.6267911858044\n",
      "trigram:  40033.6378309513\n"
     ]
    }
   ],
   "source": [
    "print(\"== TRAIN PERPLEXITY == \")\n",
    "print(\"unigram: \", calculate_unigram_perplexity(dataset_model_smoothed, Data))\n",
    "print(\"bigram: \", calculate_bigram_perplexity(dataset_model_smoothed, Data))\n",
    "print(\"trigram: \", calculate_trigram_perplexity(dataset_model_smoothed, Data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(len(test)):\n",
    "    s = test[d]\n",
    "    s = s.lower()\n",
    "    s = s.translate(table)\n",
    "    s = s.translate(remove_digits)\n",
    "    s = nltk.word_tokenize(s)\n",
    "    test[d] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== TEST PERPLEXITY == \n",
      "unigram:  98.70888683427059\n",
      "bigram:  1.0437204499672985\n",
      "trigram:  1.0000786856105486\n"
     ]
    }
   ],
   "source": [
    "print(\"== TEST PERPLEXITY == \")\n",
    "print(\"unigram: \", calculate_unigram_perplexity(dataset_model, test))\n",
    "print(\"bigram: \", calculate_bigram_perplexity(dataset_model, test))\n",
    "print(\"trigram: \", calculate_trigram_perplexity(dataset_model, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== TEST PERPLEXITY == \n",
      "unigram:  1920.9823811109472\n",
      "bigram:  5247.631636500524\n",
      "trigram:  72673.97032516371\n"
     ]
    }
   ],
   "source": [
    "print(\"== TEST PERPLEXITY == \")\n",
    "print(\"unigram: \", calculate_unigram_perplexity(dataset_model_smoothed, test))\n",
    "print(\"bigram: \", calculate_bigram_perplexity(dataset_model_smoothed, test))\n",
    "print(\"trigram: \", calculate_trigram_perplexity(dataset_model_smoothed, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_model_smoothed = TrigramLanguageModel(test,smoothing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_vocab_keys = dataset_model_smoothed.sorted_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8245"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_vocab_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Least_probability = 1.0/float(dataset_model_smoothed.unique__bigram_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Generation : Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = {}\n",
    "for x in dataset_model_smoothed.trigram_frequencies.keys():\n",
    "    if (x[0],x[1]) not in sent.keys():\n",
    "        sent[(x[0],x[1])] = {}\n",
    "    sent[(x[0],x[1])][x[2]] = float(dataset_model_smoothed.trigram_frequencies[(x[0],x[1],x[2])])/float(dataset_model_smoothed.bigram_frequencies[(x[0],x[1])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sent.keys():\n",
    "    sent[x] = dict(sorted(sent[x].items(),key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today the latest conviction in an attack that renewed longrun None None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sentence_finished = False\n",
    "text = ['today',\n",
    " 'the']\n",
    "while not sentence_finished:\n",
    "    r = random.random()\n",
    "    acc = .0\n",
    "    for word in sent[tuple(text[-2:])].keys():\n",
    "        acc += sent[tuple(text[-2:])][word]\n",
    "        if acc >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-2:] == ['None', 'None'] or len(text) > 30:\n",
    "        sentence_finished = True\n",
    "    if tuple(text[-2:]) not in sent.keys():\n",
    "        sent[tuple(text[-2:])] = {'None':Least_probability}\n",
    "        \n",
    "print (' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Generation : Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = {}\n",
    "for x in dataset_model_smoothed.bigram_frequencies.keys():\n",
    "    if x[0] not in sent1.keys():\n",
    "        sent1[x[0]] = {}\n",
    "    sent1[x[0]][x[1]] = float(dataset_model_smoothed.bigram_frequencies[(x[0],x[1])])/float(dataset_model_smoothed.unigram_frequencies[x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sent1.keys():\n",
    "    sent1[x] = dict(sorted(sent1[x].items(),key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today the us None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sentence_finished = False\n",
    "text = ['today', 'the']\n",
    "while not sentence_finished:\n",
    "    r = random.random()\n",
    "    acc = .0\n",
    "    if text[-1] not in sent1.keys():\n",
    "        break\n",
    "    for word in sent1[text[-1]].keys():\n",
    "        acc += sent1[text[-1]][word]\n",
    "        if acc >= r:\n",
    "            text.append(word)\n",
    "            break\n",
    "\n",
    "    if text[-1] == 'None' or len(text) > 30:\n",
    "        sentence_finished = True\n",
    "    if text[-1] not in sent.keys():\n",
    "        sent1[text[-1]] = {'None':Least_probability}\n",
    "        \n",
    "print (' '.join([t for t in text if t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP_Assignment_6.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
