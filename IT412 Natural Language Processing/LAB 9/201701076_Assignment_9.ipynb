{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vedant\\Desktop\\NLP LAB\\Assignment9\\english_dataset\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\Vedant\\Desktop\\NLP LAB\\Assignment9\\english_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('english_dataset.tsv',delimiter='\\t',encoding='utf-8')\n",
    "df_train = list(dataset['text'])\n",
    "dataset_test = pd.read_csv('hasoc2019_en_test-2919.tsv',delimiter='\\t',encoding='utf-8')\n",
    "df_test = list(dataset_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset[dataset['task_1'] == 'HOF']\n",
    "dataset_test1 = dataset_test[dataset_test['task_1'] == 'HOF']\n",
    "df_trains = {0:df_train,1:list(dataset1['text']),2:list(dataset1['text'])}\n",
    "df_tests = {0:df_test,1:list(dataset_test1['text']),2:list(dataset_test1['text'])}\n",
    "targets = {0:dataset['task_1'],1:dataset1['task_2'],2:dataset1['task_3']}\n",
    "targets_test = {0:dataset_test['task_1'],1:dataset_test1['task_2'],2:dataset_test1['task_3']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import unidecode\n",
    "from word2number import w2n\n",
    "from pycontractions import Contractions\n",
    "import gensim.downloader as api\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "model = api.load(\"glove-twitter-25\")\n",
    "cont = Contractions(kv_model=model)\n",
    "cont.load_models()\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    text = text.strip()\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "def expand_contractions(text):\n",
    "    text = list(cont.expand_texts([text], precise=True))[0]\n",
    "    return text\n",
    "\n",
    "def text_preprocessing(text, accented_chars=True, contractions=True, \n",
    "                       convert_num=True, extra_whitespace=True, \n",
    "                       lemmatization=True, lowercase=True, punctuations=True,\n",
    "                       remove_html=True, remove_num=True, special_chars=True, \n",
    "                       stop_words=True):\n",
    "    \n",
    "    if remove_html == True: #remove html tags\n",
    "        text = strip_html_tags(text)\n",
    "    if extra_whitespace == True: #remove extra whitespaces\n",
    "        text = remove_whitespace(text)\n",
    "    #if convert_num == True:\n",
    "    #    text = \n",
    "    #if special_chars == True:\n",
    "    #    text = \n",
    "    if punctuations == True:\n",
    "        text = re.sub('[^a-zA-Z0-9 \\n\\.]', '', text)\n",
    "    if remove_num == True:\n",
    "        text = re.sub(r'\\d', '',text)\n",
    "    if accented_chars == True: #remove accented characters\n",
    "        text = remove_accented_chars(text)\n",
    "    if contractions == True: #expand contractions\n",
    "        text = expand_contractions(text)\n",
    "    if lowercase == True: #convert all characters to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "    tknzr = TweetTokenizer()\n",
    "    doc = tknzr.tokenize(text) #tokenise text\n",
    "    \n",
    "    if stop_words == True:\n",
    "        for w in doc:\n",
    "            if w in stop:\n",
    "                doc.remove(w)\n",
    "    if lemmatization == True:\n",
    "        wnl = WordNetLemmatizer()\n",
    "        doc = [wnl.lemmatize(i) for i in doc] \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = []\n",
    "for text in df_train:\n",
    "    for x in text.split(\" \"):\n",
    "        Vocab.append(x)\n",
    "Vocab = list(set(Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(3):\n",
    "    for i in range(len(df_trains[j])):\n",
    "        df_trains[j][i] = text_preprocessing(df_trains[j][i])\n",
    "        df_trains[j][i] = \" \".join(df_trains[j][i])\n",
    "    for i in range(len(df_tests[j])):\n",
    "        df_tests[j][i] = text_preprocessing(df_tests[j][i])\n",
    "        df_tests[j][i] = \" \".join(df_tests[j][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer = 'word',vocabulary = Vocab)\n",
    "tfidf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.66      0.50       288\n",
      "           1       0.86      0.66      0.75       865\n",
      "\n",
      "    accuracy                           0.66      1153\n",
      "   macro avg       0.63      0.66      0.62      1153\n",
      "weighted avg       0.74      0.66      0.68      1153\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.67      0.61       124\n",
      "           1       0.27      0.10      0.14        71\n",
      "           2       0.51      0.63      0.57        93\n",
      "\n",
      "    accuracy                           0.52       288\n",
      "   macro avg       0.45      0.47      0.44       288\n",
      "weighted avg       0.48      0.52      0.48       288\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90       245\n",
      "           1       0.15      0.05      0.07        43\n",
      "\n",
      "    accuracy                           0.82       288\n",
      "   macro avg       0.50      0.50      0.49       288\n",
      "weighted avg       0.75      0.82      0.78       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    X_train_counts = count_vect.fit_transform(df_trains[i])\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    X_test_counts = count_vect.fit_transform(df_tests[i])\n",
    "    X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(targets[i]))\n",
    "    targets[i] = le.transform(targets[i].astype(str))\n",
    "    clf = SGDClassifier().fit(X_train_tfidf, targets[i])\n",
    "    predicted = clf.predict(X_test_tfidf)\n",
    "    targets_test[i] = le.transform(targets_test[i].astype(str))\n",
    "    print(classification_report(targets_test[i], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.61      0.48       288\n",
      "           1       0.84      0.69      0.76       865\n",
      "\n",
      "    accuracy                           0.67      1153\n",
      "   macro avg       0.62      0.65      0.62      1153\n",
      "weighted avg       0.73      0.67      0.69      1153\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.74      0.63       124\n",
      "           1       0.41      0.10      0.16        71\n",
      "           2       0.53      0.59      0.56        93\n",
      "\n",
      "    accuracy                           0.53       288\n",
      "   macro avg       0.50      0.48      0.45       288\n",
      "weighted avg       0.51      0.53      0.49       288\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92       245\n",
      "           1       0.00      0.00      0.00        43\n",
      "\n",
      "    accuracy                           0.85       288\n",
      "   macro avg       0.43      0.50      0.46       288\n",
      "weighted avg       0.72      0.85      0.78       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    X_train_counts = count_vect.fit_transform(df_trains[i])\n",
    "    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "    X_test_counts = count_vect.fit_transform(df_tests[i])\n",
    "    X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(targets[i]))\n",
    "    targets[i] = le.transform(targets[i].astype(str))\n",
    "    clf = LinearSVC().fit(X_train_tfidf, targets[i])\n",
    "    predicted = clf.predict(X_test_tfidf)\n",
    "    targets_test[i] = le.transform(targets_test[i].astype(str))\n",
    "    print(classification_report(targets_test[i], predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pretrained Embeddings : GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/Vedant/Desktop/NLP LAB/Assignment9/glove.6B.100d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(3):\n",
    "    for i in range(len(df_trains[j])):\n",
    "        df_trains[j][i] = text_preprocessing(df_trains[j][i])\n",
    "        e = np.zeros((1,100))\n",
    "        for word in df_trains[j][i]:\n",
    "            try:\n",
    "                e = e + embeddings_dict[word]\n",
    "            except:\n",
    "                pass\n",
    "        df_trains[j][i] = e/len(df_trains[j][i])\n",
    "    for i in range(len(df_tests[j])):\n",
    "        df_tests[j][i] = text_preprocessing(df_tests[j][i])\n",
    "        e = np.zeros((1,100))\n",
    "        for word in df_tests[j][i]:\n",
    "            try:\n",
    "                e = e + embeddings_dict[word]\n",
    "            except:\n",
    "                pass\n",
    "        df_tests[j][i] = e/len(df_tests[j][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trains[0] = np.array(df_trains[0]).reshape(-1,100)\n",
    "df_trains[1] = np.array(df_trains[1]).reshape(-1,100)\n",
    "df_trains[2] = np.array(df_trains[2]).reshape(-1,100)\n",
    "df_tests[0] = np.array(df_tests[0]).reshape(-1,100)\n",
    "df_tests[1] = np.array(df_tests[1]).reshape(-1,100)\n",
    "df_tests[2] = np.array(df_tests[2]).reshape(-1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.47      0.45       288\n",
      "           1       0.82      0.79      0.80       865\n",
      "\n",
      "    accuracy                           0.71      1153\n",
      "   macro avg       0.62      0.63      0.63      1153\n",
      "weighted avg       0.72      0.71      0.72      1153\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.87      0.66       124\n",
      "           1       0.40      0.03      0.05        71\n",
      "           2       0.63      0.53      0.57        93\n",
      "\n",
      "    accuracy                           0.55       288\n",
      "   macro avg       0.52      0.48      0.43       288\n",
      "weighted avg       0.53      0.55      0.48       288\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.99      0.91       245\n",
      "           1       0.00      0.00      0.00        43\n",
      "\n",
      "    accuracy                           0.84       288\n",
      "   macro avg       0.42      0.49      0.46       288\n",
      "weighted avg       0.72      0.84      0.78       288\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(list(targets[i]))\n",
    "    targets[i] = le.transform(targets[i].astype(str))\n",
    "    clf = LinearSVC().fit(df_trains[i], targets[i])\n",
    "    predicted = clf.predict(df_tests[i])\n",
    "    targets_test[i] = le.transform(targets_test[i].astype(str))\n",
    "    print(classification_report(targets_test[i], predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
